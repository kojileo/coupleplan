# Geminiモデル選択ガイド

## 概要

CouplePlanでは、2つのGeminiモデル選択肢があります。ビジネス要件に応じて最適なモデルを選択してください。

---

## 🎯 2つの選択肢

### オプションA: gemini-1.5-flash-latest（推奨）⭐

**コスト効率重視・標準品質**

```env
AI_MODEL=gemini-1.5-flash-latest
AI_MAX_TOKENS=2000
```

| 項目                   | 値                      |
| ---------------------- | ----------------------- |
| **トークン使用量**     | 1,000-1,500             |
| **生成時間**           | 10-15秒                 |
| **品質**               | 高品質                  |
| **思考モード**         | なし                    |
| **1日の処理能力**      | 1,000-1,500リクエスト   |
| **月間処理能力**       | 30,000-45,000リクエスト |
| **対応可能ユーザー数** | 10,000-50,000           |

#### メリット

- ✅ **トークン効率最高**（コスト最小）
- ✅ **高速レスポンス**（10-15秒）
- ✅ **大規模対応**（無料枠で数万ユーザー）
- ✅ **高品質な出力**（実用レベル）
- ✅ **タイムアウトリスク低**

#### デメリット

- ⚠️ 思考モードなし
- ⚠️ 超複雑な推論は苦手

#### 推奨する場合

- ✅ 本番環境
- ✅ スケールを重視
- ✅ コスト削減を重視
- ✅ 10,000ユーザー以上を目指す

---

### オプションB: gemini-2.5-flash（高度な推論）

**品質最優先・思考モード**

```env
AI_MODEL=gemini-2.5-flash
AI_MAX_TOKENS=5000
```

| 項目                   | 値                     |
| ---------------------- | ---------------------- |
| **トークン使用量**     | 4,000-5,000            |
| **生成時間**           | 25-35秒                |
| **品質**               | 最高品質               |
| **思考モード**         | あり（約2000トークン） |
| **1日の処理能力**      | 300-375リクエスト      |
| **月間処理能力**       | 9,000-11,250リクエスト |
| **対応可能ユーザー数** | 1,000-3,000            |

#### メリット

- ✅ **最高品質の出力**
- ✅ **高度な推論能力**（複雑な問題解決）
- ✅ **思考プロセス**による深い分析

#### デメリット

- ❌ **トークン使用量が3-4倍**
- ❌ **生成時間が2-3倍**（25-35秒）
- ❌ **処理能力が1/3**
- ❌ **タイムアウトリスク高**
- ❌ **大規模には不向き**

#### 推奨する場合

- ✅ 品質を最優先
- ✅ ユーザー数が少ない（数百人規模）
- ✅ 複雑な推論が必要
- ✅ コストは気にしない

---

## 📊 詳細比較

### トークン使用量

| モデル        | プロンプト | 思考 | 出力      | 合計             |
| ------------- | ---------- | ---- | --------- | ---------------- |
| **1.5-flash** | 95         | 0    | 900-1400  | **1000-1500** ✅ |
| 2.5-flash     | 95         | 2000 | 1900-2900 | **4000-5000** ❌ |

### 処理能力（無料枠: 1日1,500リクエスト）

| モデル        | 1日         | 月間          | 年間            |
| ------------- | ----------- | ------------- | --------------- |
| **1.5-flash** | 1,000-1,500 | 30,000-45,000 | 360,000-540,000 |
| 2.5-flash     | 300-375     | 9,000-11,250  | 108,000-135,000 |

**差**: 約3-4倍の処理能力差

### 対応可能ユーザー数

**想定**: 1ユーザーあたり月3プラン生成

| モデル        | 月間リクエスト | 対応ユーザー数       |
| ------------- | -------------- | -------------------- |
| **1.5-flash** | 30,000-45,000  | **10,000-15,000** ✅ |
| 2.5-flash     | 9,000-11,250   | **3,000-3,750** ⚠️   |

### コスト（無料枠超過時）

仮に有料プランに移行した場合の推定コスト:

| モデル        | 100,000リクエスト/月 | コスト差      |
| ------------- | -------------------- | ------------- |
| **1.5-flash** | $20-30               | -             |
| 2.5-flash     | $80-120              | **3-4倍高い** |

---

## 🎯 推奨される選択

### 🟢 オプションA推奨（90%のケース）

以下に該当する場合はオプションA:

- ✅ 本番環境での運用
- ✅ 1,000ユーザー以上を目指す
- ✅ コストを抑えたい
- ✅ 高速レスポンスが必要
- ✅ 標準的な品質で十分

**設定**:

```env
AI_MODEL=gemini-1.5-flash-latest
AI_MAX_TOKENS=2000
```

### 🟡 オプションB推奨（10%のケース）

以下に該当する場合はオプションB:

- ✅ 最高品質が絶対条件
- ✅ ユーザー数が少ない（数百人規模）
- ✅ 複雑な推論が必要
- ✅ 生成時間は気にしない
- ✅ コストは気にしない

**設定**:

```env
AI_MODEL=gemini-2.5-flash
AI_MAX_TOKENS=5000
```

---

## 🛠️ オプションBの完全な設定

gemini-2.5-flashを使用する場合の完全な設定:

### 1. 環境変数（`.env.local`）

```env
AI_PROVIDER=gemini
GEMINI_API_KEY=your_gemini_api_key_here
AI_MODEL=gemini-2.5-flash
AI_MAX_TOKENS=5000
AI_TEMPERATURE=0.7
```

### 2. タイムアウト設定

**自動調整されます**:

- AI_MAX_TOKENS=5000 → Gemini APIタイムアウト60秒
- レート制限タイムアウト90秒

### 3. 本番環境（Vercel）

| 変数名           | 値                 |
| ---------------- | ------------------ |
| `AI_PROVIDER`    | `gemini`           |
| `GEMINI_API_KEY` | (APIキー)          |
| `AI_MODEL`       | `gemini-2.5-flash` |
| `AI_MAX_TOKENS`  | `5000`             |
| `AI_TEMPERATURE` | `0.7`              |

---

## ⚖️ トレードオフの詳細

### トークン使用量のトレードオフ

**オプションA（1.5-flash）**:

```
1リクエスト = 1,200トークン
1日1,500リクエスト可能
→ 月間45,000リクエスト対応
→ 15,000ユーザー対応可能（1人3プラン/月）
```

**オプションB（2.5-flash）**:

```
1リクエスト = 4,500トークン
1日1,500リクエストだが、実質333リクエスト相当
→ 月間10,000リクエスト対応
→ 3,300ユーザー対応可能（1人3プラン/月）
```

### 生成時間のトレードオフ

**オプションA**: 10-15秒

- ユーザー満足度: 高い
- 離脱率: 低い

**オプションB**: 25-35秒

- ユーザー満足度: 中程度
- 離脱率: やや高い可能性

### 品質のトレードオフ

**オプションA（1.5-flash）**:

- 品質: 85-90点
- 実用性: 十分
- 推論: 標準的

**オプションB（2.5-flash）**:

- 品質: 95-98点
- 実用性: 非常に高い
- 推論: 高度

**差**: 5-10点の品質差

---

## 💡 判断基準

### ビジネス視点

| 判断軸                             | オプションA | オプションB |
| ---------------------------------- | ----------- | ----------- |
| **初期MVPフェーズ**                | ⭐⭐⭐      | ⭐          |
| **成長期（1,000-10,000ユーザー）** | ⭐⭐⭐      | ⭐          |
| **プレミアムサービス**             | ⭐⭐        | ⭐⭐⭐      |
| **無料プラン**                     | ⭐⭐⭐      | ❌          |

### 技術視点

| 判断軸               | オプションA | オプションB |
| -------------------- | ----------- | ----------- |
| **スケーラビリティ** | ⭐⭐⭐      | ⭐          |
| **レスポンス速度**   | ⭐⭐⭐      | ⭐          |
| **コスト効率**       | ⭐⭐⭐      | ⭐          |
| **品質**             | ⭐⭐        | ⭐⭐⭐      |

---

## 🚀 推奨戦略

### ハイブリッド戦略（将来的に検討）

1. **無料プラン**: gemini-1.5-flash-latest
   - トークン効率重視
   - 高速レスポンス

2. **プレミアムプラン**: gemini-2.5-flash
   - 最高品質
   - 高度な推論

この戦略で、コストを抑えつつプレミアム価値を提供可能。

---

## 📋 実装の柔軟性

修正により、両方のモデルに対応できるようになりました：

### 動的タイムアウト調整 ✅

```typescript
function getTimeoutForTokens(maxTokens: number): number {
  if (maxTokens <= 2000) return 30000; // 30秒
  if (maxTokens <= 4000) return 45000; // 45秒
  if (maxTokens <= 6000) return 60000; // 60秒
  return 90000; // 90秒
}
```

### 柔軟なトークン設定 ✅

```typescript
maxOutputTokens: config.maxTokens || 2000; // ユーザー設定を尊重
```

---

## ✨ 最終推奨

### 🎯 本番環境での推奨設定

```env
AI_PROVIDER=gemini
GEMINI_API_KEY=your_gemini_api_key_here
AI_MODEL=gemini-1.5-flash-latest
AI_MAX_TOKENS=2000
AI_TEMPERATURE=0.7
```

**理由**:

1. **コスト効率**: 無料枠で10,000ユーザー対応可能
2. **スケーラビリティ**: 月間45,000リクエスト処理
3. **ユーザー体験**: 10-15秒の高速レスポンス
4. **品質**: 実用レベルで十分高品質
5. **リスク**: タイムアウトほぼなし

### 代替案（品質最優先の場合）

```env
AI_MODEL=gemini-2.5-flash
AI_MAX_TOKENS=5000
```

**注意事項**:

- ⚠️ 処理能力が1/3に
- ⚠️ 生成時間が2-3倍に
- ⚠️ 3,000ユーザーまでしか対応不可

---

## 📊 ユースケース別推奨

| ユースケース                 | 推奨モデル   | 理由             |
| ---------------------------- | ------------ | ---------------- |
| **スタートアップ・MVP**      | 1.5-flash ⭐ | スケーラビリティ |
| **小規模サービス（<1,000）** | どちらでも   | お好みで         |
| **成長期（1,000-10,000）**   | 1.5-flash ⭐ | コスト効率       |
| **大規模（10,000+）**        | 1.5-flash ⭐ | 必須             |
| **プレミアムサービス**       | 2.5-flash    | 品質重視         |
| **エンタープライズ**         | 1.5-flash ⭐ | スケール必須     |

---

## 🔄 モデル切り替え方法

### 簡単に切り替え可能

環境変数を変更するだけ:

```env
# オプションA
AI_MODEL=gemini-1.5-flash-latest
AI_MAX_TOKENS=2000

# オプションB
AI_MODEL=gemini-2.5-flash
AI_MAX_TOKENS=5000
```

### A/Bテストも可能

将来的に、ユーザーごとに異なるモデルを使用することも可能:

```typescript
const model = user.isPremium
  ? 'gemini-2.5-flash' // プレミアム: 高品質
  : 'gemini-1.5-flash-latest'; // 無料: 標準品質
```

---

## 💰 コスト試算

### 無料枠での運用

| モデル        | 月間処理 | 対応ユーザー | コスト  |
| ------------- | -------- | ------------ | ------- |
| **1.5-flash** | 45,000   | 15,000       | **0円** |
| 2.5-flash     | 11,250   | 3,750        | **0円** |

### 有料プラン移行時（仮想）

**月間100,000リクエストの場合**:

| モデル        | トークン総数 | 推定コスト |
| ------------- | ------------ | ---------- |
| **1.5-flash** | 120-150M     | $20-30     |
| 2.5-flash     | 400-500M     | $80-120    |

**差**: 約3-4倍のコスト差

---

## 🎯 結論

### 90%のケースでオプションA推奨

**gemini-1.5-flash-latest + 2000トークン**が最適:

1. ✅ コスト効率最高
2. ✅ 高速レスポンス
3. ✅ 大規模対応可能
4. ✅ 十分な品質
5. ✅ 無料枠で長期運用可能

### 10%のケースでオプションB

品質を最優先し、スケールは気にしない場合のみ:

1. ⚠️ 最高品質
2. ⚠️ 複雑な推論
3. ⚠️ 小規模運用のみ

---

## 🚀 推奨アクション

### すぐに実行

`.env.local` を確認・更新:

```env
# 推奨設定
AI_MODEL=gemini-1.5-flash-latest
AI_MAX_TOKENS=2000
```

### 開発サーバーを再起動

```bash
npm run dev
```

### 動作確認

期待される結果:

- 生成時間: 10-15秒
- トークン使用: 1000-1500
- エラー: なし

---

**最終更新**: 2025年10月9日  
**バージョン**: v0.4.2  
**推奨**: オプションA（gemini-1.5-flash-latest）
